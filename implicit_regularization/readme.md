# Implicit Regularization in Deep Neural Networks: An Optimization Perspective

**Author:** Kamaljeet Singh  
PhD Student, Statistics & Data Science  
The University of Arizona  
Email: kamaljeetsingh@arizona.edu

## Overview

This project is a literature review exploring **implicit regularization** in **deep neural networks (DNNs)** from an **optimization perspective**. It focuses on how overparameterized models—those with more parameters than training data—can still generalize well to unseen data, despite fitting the training data perfectly.

## Key Topics

- Implicit regularization through optimization algorithms (e.g., SGD)
- Overparameterization and generalization
- Adaptive optimizers vs. SGD
- The role of mini-batch training


## Project Contents

- `paper/` – Literature review PDF (final report)
- `code/` – (Optional) Experimental or illustrative code

## Motivation

Despite the highly non-convex nature of training deep networks, gradient-based optimizers often find solutions that generalize well. This project investigates the underlying optimization dynamics and how they act as an implicit form of regularization.

